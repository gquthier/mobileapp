import * as FileSystem from 'expo-file-system/legacy';
import Constants from 'expo-constants';
import { supabase } from '../lib/supabase';
import { RealtimeTranscriptionService, RealtimeTranscriptionOptions } from './realtimeTranscriptionService';

export interface TranscriptionSegment {
  id: string;
  seek: number;
  start: number;
  end: number;
  text: string;
  tokens: number[];
  temperature: number;
  avg_logprob: number;
  compression_ratio: number;
  no_speech_prob: number;
}

export interface TranscriptionResult {
  task: 'transcribe';
  language: string;
  duration: number;
  text: string;
  segments: TranscriptionSegment[];
}

export interface TranscriptionOptions {
  language?: string;
  prompt?: string;
  response_format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt';
  temperature?: number;
}

export interface StoredTranscription {
  id: string;
  video_id: string;
  text: string;
  segments: TranscriptionSegment[];
  language: string;
  duration: number;
  created_at: string;
  updated_at: string;
}

export class TranscriptionService {
  private static readonly OPENAI_API_URL = 'https://api.openai.com/v1/audio/transcriptions';
  private static readonly MAX_FILE_SIZE = 25 * 1024 * 1024; // 25MB limit for OpenAI
  private static readonly SUPPORTED_FORMATS = ['.mp3', '.mp4', '.mpeg', '.mpga', '.m4a', '.wav', '.webm', '.mov'];

  // ISO-639-1 language codes supported by OpenAI Whisper
  private static readonly SUPPORTED_LANGUAGES = [
    'af', 'am', 'ar', 'as', 'az', 'ba', 'be', 'bg', 'bn', 'bo', 'br', 'bs', 'ca', 'cs', 'cy', 'da', 'de', 'el', 'en', 'es', 'et', 'eu', 'fa', 'fi', 'fo', 'fr', 'gl', 'gu', 'ha', 'haw', 'he', 'hi', 'hr', 'ht', 'hu', 'hy', 'id', 'is', 'it', 'ja', 'jw', 'ka', 'kk', 'km', 'kn', 'ko', 'la', 'lb', 'ln', 'lo', 'lt', 'lv', 'mg', 'mi', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'ne', 'nl', 'nn', 'no', 'oc', 'pa', 'pl', 'ps', 'pt', 'ro', 'ru', 'sa', 'sd', 'si', 'sk', 'sl', 'sn', 'so', 'sq', 'sr', 'su', 'sv', 'sw', 'ta', 'te', 'tg', 'th', 'tk', 'tl', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'yi', 'yo', 'zh'
  ];

  /**
   * Download file from URL to local storage for OpenAI API
   */
   private static async downloadFileLocally(fileUrl: string): Promise<string> {
    try {
      console.log('üì• Downloading file from URL:', fileUrl);

      // Generate local file path
      const timestamp = Date.now();
      const fileExtension = fileUrl.includes('.mp4') ? '.mp4' : '.m4a';
      const localPath = `${FileSystem.cacheDirectory}temp_audio_${timestamp}${fileExtension}`;

      // Download the file
      const downloadResult = await FileSystem.downloadAsync(fileUrl, localPath);

      if (downloadResult.status !== 200) {
        throw new Error(`Failed to download file: ${downloadResult.status}`);
      }

      console.log('‚úÖ File downloaded successfully:', {
        url: fileUrl,
        localPath: downloadResult.uri,
        size: downloadResult.headers['Content-Length'] || 'unknown'
      });

      return downloadResult.uri;
    } catch (error) {
      console.error('‚ùå Failed to download file:', error);
      throw error;
    }
  }

  /**
   * Validate and normalize language code for OpenAI Whisper API
   */
  private static validateLanguage(language?: string): string | undefined {
    if (!language) {
      return undefined; // Auto-detect language
    }

    // Handle special cases
    if (language === 'auto' || language === 'detect') {
      // Use French as default instead of auto-detect (OpenAI doesn't accept 'auto')
      console.log('üá´üá∑ Using French as default language (auto ‚Üí fr)');
      return 'fr';
    }

    // Convert to lowercase and check if supported
    const normalizedLang = language.toLowerCase();
    if (this.SUPPORTED_LANGUAGES.includes(normalizedLang)) {
      return normalizedLang;
    }

    // Handle common language variations
    const languageMap: { [key: string]: string } = {
      'fr-fr': 'fr', 'fr-ca': 'fr',
      'en-us': 'en', 'en-gb': 'en', 'en-au': 'en', 'en-ca': 'en',
      'es-es': 'es', 'es-mx': 'es', 'es-ar': 'es',
      'de-de': 'de', 'de-at': 'de', 'de-ch': 'de',
      'it-it': 'it',
      'pt-br': 'pt', 'pt-pt': 'pt',
      'zh-cn': 'zh', 'zh-tw': 'zh',
      'ar-sa': 'ar'
    };

    const mappedLang = languageMap[normalizedLang];
    if (mappedLang && this.SUPPORTED_LANGUAGES.includes(mappedLang)) {
      return mappedLang;
    }

    console.warn(`‚ö†Ô∏è Unsupported language '${language}', using auto-detection instead`);
    return undefined; // Fall back to auto-detection
  }

  /**
   * Get OpenAI API key from environment
   */
  private static getApiKey(): string {
    // Try multiple ways to get the API key
    let apiKey = process.env.EXPO_PUBLIC_OPENAI_API_KEY ||
                 Constants.expoConfig?.extra?.OPENAI_API_KEY ||
                 process.env.OPENAI_API_KEY;

    if (!apiKey) {
      throw new Error('OpenAI API key not found. Please set EXPO_PUBLIC_OPENAI_API_KEY in environment variables.');
    }
    return apiKey;
  }

  // REMOVED: Complex validation method - using inline validation in transcribeVideo instead

  /**
   * Transcribe video file using OpenAI Whisper API - VERSION CORRIG√âE
   */
  static async transcribeVideo(
    videoFilePath: string,
    options: TranscriptionOptions = {}
  ): Promise<TranscriptionResult> {
    try {
      console.log('üé• IMPROVED transcription starting:', { videoFilePath });

      let localFilePath = videoFilePath;

      // Step 1: Download if URL
      if (videoFilePath.startsWith('http://') || videoFilePath.startsWith('https://')) {
        console.log('‚¨áÔ∏è Downloading file from URL...');
        localFilePath = await this.downloadFileLocally(videoFilePath);
        console.log('‚úÖ File downloaded to:', localFilePath);
      }

      // NOUVEAU: D√©tection du format et tentative de conversion si n√©cessaire
      const originalExtension = videoFilePath.toLowerCase().substring(videoFilePath.lastIndexOf('.'));
      console.log('üé¨ Original file extension:', originalExtension);

      // Pour les formats vid√©o (MP4, MOV), on va essayer d'abord tel quel,
      // puis avec des param√®tres diff√©rents si √ßa √©choue
      const isVideoFormat = ['.mp4', '.mov', '.mpeg'].includes(originalExtension);
      console.log('üìπ Is video format:', isVideoFormat);

      // Step 2: Validate file exists and size (RENFORC√â)
      const fileInfo = await FileSystem.getInfoAsync(localFilePath);
      if (!fileInfo.exists) {
        throw new Error('File does not exist at path: ' + localFilePath);
      }

      if (!fileInfo.size || fileInfo.size === 0) {
        throw new Error('File is empty or size could not be determined');
      }

      const fileSizeMB = fileInfo.size / 1024 / 1024;
      console.log('üìÅ File validation:', {
        exists: fileInfo.exists,
        size: `${fileSizeMB.toFixed(2)}MB`,
        path: localFilePath
      });

      // V√âRIFICATION: Limite de 25MB pour OpenAI Whisper
      if (fileSizeMB > 25) {
        throw new Error(`File too large for OpenAI Whisper: ${fileSizeMB.toFixed(2)}MB (max: 25MB)`);
      }

      // NOUVEAU: V√©rification de la dur√©e minimale (0.1 secondes selon OpenAI)
      // Pour les fichiers tr√®s petits, on peut estimer grossi√®rement la dur√©e
      if (fileSizeMB < 0.01) { // Moins de 10KB = probablement tr√®s court
        console.warn('‚ö†Ô∏è File might be too short for transcription (min 0.1s required by OpenAI)');
      }

      // Step 3: Get API key
      const apiKey = this.getApiKey();
      console.log('üîë API key available:', !!apiKey);

      // Step 4: Prepare parameters for retry logic

      // Mapping des extensions vers les types MIME corrects
      const mimeTypeMap: { [key: string]: string } = {
        '.mp4': 'video/mp4',
        '.mov': 'video/quicktime',
        '.m4a': 'audio/mp4',
        '.mp3': 'audio/mpeg',
        '.wav': 'audio/wav',
        '.webm': 'video/webm',
        '.mpeg': 'video/mpeg',
        '.mpga': 'audio/mpeg'
      };

      const mimeType = mimeTypeMap[originalExtension] || 'video/mp4';
      const fileName = `video${originalExtension}`;

      console.log('üìÑ MIME Type:', mimeType, '| Filename:', fileName);
      console.log('üì§ Starting transcription with retry logic...');

      // NOUVEAU: Fonction pour tenter la transcription avec retry et diff√©rents formats
      return await this.attemptTranscriptionWithRetry(
        localFilePath,
        apiKey,
        mimeType,
        fileName,
        options,
        isVideoFormat
      );
    } catch (error) {
      console.error('‚ùå TRANSCRIPTION FAILED:', error);
      throw error;
    }
  }

  /**
   * NOUVELLE M√âTHODE: Tentative de transcription avec retry automatique
   */
  private static async attemptTranscriptionWithRetry(
    localFilePath: string,
    apiKey: string,
    mimeType: string,
    fileName: string,
    options: TranscriptionOptions,
    isVideoFormat: boolean
  ): Promise<TranscriptionResult> {
    // Configuration des tentatives avec diff√©rents formats
    const attempts = [
      // Tentative 1: Format original
      { mimeType, fileName, description: 'format original' },
      // Tentative 2: Si vid√©o, essayer comme audio MP4
      ...(isVideoFormat ? [
        { mimeType: 'audio/mp4', fileName: fileName.replace(/\.(mp4|mov|mpeg)$/i, '.m4a'), description: 'audio MP4' },
        { mimeType: 'audio/mpeg', fileName: fileName.replace(/\.(mp4|mov|mpeg)$/i, '.mp3'), description: 'audio MPEG' }
      ] : [])
    ];

    let lastError: Error | null = null;

    for (let i = 0; i < attempts.length; i++) {
      const attempt = attempts[i];
      console.log(`üîÑ Tentative ${i + 1}/${attempts.length}: ${attempt.description}`);

      try {
        const result = await this.makeTranscriptionRequest(
          localFilePath,
          apiKey,
          attempt.mimeType,
          attempt.fileName,
          options
        );

        console.log(`‚úÖ SUCC√àS avec ${attempt.description}!`);
        return result;

      } catch (error) {
        console.log(`‚ùå √âchec tentative ${i + 1}: ${error.message}`);
        lastError = error as Error;

        // Si c'est une erreur de format et qu'on a d'autres tentatives, continuer
        if (error.message.includes('Invalid file format') && i < attempts.length - 1) {
          continue;
        }

        // Pour les autres erreurs (401, 429, etc.), pas la peine de retry
        if (!error.message.includes('Invalid file format')) {
          throw error;
        }
      }
    }

    // Si toutes les tentatives ont √©chou√©
    throw lastError || new Error('All transcription attempts failed');
  }

  /**
   * NOUVELLE M√âTHODE: Effectue une requ√™te de transcription
   */
  private static async makeTranscriptionRequest(
    localFilePath: string,
    apiKey: string,
    mimeType: string,
    fileName: string,
    options: TranscriptionOptions
  ): Promise<TranscriptionResult> {
    const formData = new FormData();

    formData.append('file', {
      uri: localFilePath,
      type: mimeType,
      name: fileName,
    });

    formData.append('model', 'whisper-1');

    // Param√®tres optionnels
    if (options.language && options.language !== 'auto') {
      const validLang = this.validateLanguage(options.language);
      if (validLang) {
        formData.append('language', validLang);
      }
    }

    if (options.response_format) {
      formData.append('response_format', options.response_format);
    }

    if (options.temperature !== undefined) {
      formData.append('temperature', options.temperature.toString());
    }

    console.log('üì§ Request params:', { mimeType, fileName, language: options.language });

    // Step 5: Make request (HEADERS CORRIG√âS pour React Native)
    const response = await fetch(this.OPENAI_API_URL, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        // NOTE: Ne PAS d√©finir Content-Type avec FormData en React Native
        // Le navigateur/React Native le d√©finit automatiquement avec boundary
      },
      body: formData,
    });

      console.log('üì® Response status:', response.status, response.statusText);

      if (!response.ok) {
        const errorText = await response.text();
        console.error('‚ùå OPENAI API ERROR:', {
          status: response.status,
          statusText: response.statusText,
          error: errorText,
          mimeType: mimeType,
          fileName: fileName,
        });

        // Diagnostics sp√©cifiques bas√©s sur le statut
        let errorMessage = `OpenAI API Error ${response.status}`;
        if (response.status === 400) {
          errorMessage += ': Invalid file format or parameters. ';
          if (errorText.includes('format is not supported')) {
            errorMessage += 'File format not supported by Whisper API.';
          } else if (errorText.includes('could not be decoded')) {
            errorMessage += 'File is corrupted or not a valid media file.';
          }
        } else if (response.status === 401) {
          errorMessage += ': Invalid or expired API key.';
        } else if (response.status === 413) {
          errorMessage += ': File too large (max 25MB).';
        } else if (response.status === 429) {
          errorMessage += ': Rate limit exceeded. Please wait and try again.';
        } else {
          errorMessage += ': ' + errorText;
        }

        throw new Error(errorMessage);
      }

      const result: TranscriptionResult = await response.json();
      console.log('‚úÖ SUCCESS! Text length:', result.text.length);

      return result;
    } catch (error) {
      console.error('‚ùå TRANSCRIPTION FAILED:', error);
      throw error;
    }
  }

  /**
   * Store transcription in database
   */
  static async storeTranscription(
    videoId: string,
    transcriptionResult: TranscriptionResult
  ): Promise<StoredTranscription> {
    try {
      console.log('üíæ Storing transcription in database:', { videoId });

      const transcriptionData = {
        video_id: videoId,
        text: transcriptionResult.text,
        segments: transcriptionResult.segments || [],
        language: transcriptionResult.language,
        duration: transcriptionResult.duration,
      };

      const { data, error } = await supabase
        .from('transcriptions')
        .insert(transcriptionData)
        .select()
        .single();

      if (error) {
        console.error('‚ùå Database error storing transcription:', error);
        throw error;
      }

      console.log('‚úÖ Transcription stored successfully:', data.id);
      return data;
    } catch (error) {
      console.error('‚ùå Failed to store transcription:', error);
      throw error;
    }
  }

  /**
   * Get transcription for a video
   */
  static async getTranscription(videoId: string): Promise<StoredTranscription | null> {
    try {
      console.log('üìñ Retrieving transcription for video:', videoId);

      const { data, error } = await supabase
        .from('transcriptions')
        .select('*')
        .eq('video_id', videoId)
        .single();

      if (error) {
        if (error.code === 'PGRST116') {
          // No transcription found
          return null;
        }
        console.error('‚ùå Database error retrieving transcription:', error);
        throw error;
      }

      console.log('‚úÖ Transcription retrieved successfully');
      return data;
    } catch (error) {
      console.error('‚ùå Failed to retrieve transcription:', error);
      throw error;
    }
  }

  /**
   * Search transcriptions by text
   */
  static async searchTranscriptions(
    query: string,
    userId?: string
  ): Promise<StoredTranscription[]> {
    try {
      console.log('üîç Searching transcriptions:', { query, userId });

      let queryBuilder = supabase
        .from('transcriptions')
        .select(`
          *,
          videos (
            id,
            title,
            created_at,
            user_id
          )
        `)
        .textSearch('text', query, {
          type: 'websearch',
          config: 'english'
        });

      if (userId) {
        queryBuilder = queryBuilder.eq('videos.user_id', userId);
      }

      const { data, error } = await queryBuilder;

      if (error) {
        console.error('‚ùå Database error searching transcriptions:', error);
        throw error;
      }

      console.log('‚úÖ Found transcriptions:', data?.length || 0);
      return data || [];
    } catch (error) {
      console.error('‚ùå Failed to search transcriptions:', error);
      throw error;
    }
  }

  /**
   * Delete transcription
   */
  static async deleteTranscription(transcriptionId: string): Promise<void> {
    try {
      console.log('üóëÔ∏è Deleting transcription:', transcriptionId);

      const { error } = await supabase
        .from('transcriptions')
        .delete()
        .eq('id', transcriptionId);

      if (error) {
        console.error('‚ùå Database error deleting transcription:', error);
        throw error;
      }

      console.log('‚úÖ Transcription deleted successfully');
    } catch (error) {
      console.error('‚ùå Failed to delete transcription:', error);
      throw error;
    }
  }

  /**
   * Get transcription statistics for a user
   */
  static async getTranscriptionStats(userId: string): Promise<{
    totalTranscriptions: number;
    totalDuration: number;
    languageDistribution: { [language: string]: number };
    recentTranscriptions: StoredTranscription[];
  }> {
    try {
      console.log('üìä Getting transcription statistics for user:', userId);

      // Get all transcriptions for user
      const { data, error } = await supabase
        .from('transcriptions')
        .select(`
          *,
          videos!inner (
            user_id
          )
        `)
        .eq('videos.user_id', userId)
        .order('created_at', { ascending: false });

      if (error) {
        console.error('‚ùå Database error getting transcription stats:', error);
        throw error;
      }

      const transcriptions = data || [];

      // Calculate statistics
      const totalTranscriptions = transcriptions.length;
      const totalDuration = transcriptions.reduce((sum, t) => sum + (t.duration || 0), 0);

      const languageDistribution: { [language: string]: number } = {};
      transcriptions.forEach(t => {
        languageDistribution[t.language] = (languageDistribution[t.language] || 0) + 1;
      });

      const recentTranscriptions = transcriptions.slice(0, 5);

      const stats = {
        totalTranscriptions,
        totalDuration,
        languageDistribution,
        recentTranscriptions,
      };

      console.log('‚úÖ Transcription statistics calculated:', {
        total: totalTranscriptions,
        duration: `${(totalDuration / 60).toFixed(1)} minutes`,
        languages: Object.keys(languageDistribution).length,
      });

      return stats;
    } catch (error) {
      console.error('‚ùå Failed to get transcription statistics:', error);
      throw error;
    }
  }

  /**
   * Update transcription text (for manual corrections)
   */
  static async updateTranscription(
    transcriptionId: string,
    updates: Partial<Pick<StoredTranscription, 'text' | 'segments'>>
  ): Promise<StoredTranscription> {
    try {
      console.log('‚úèÔ∏è Updating transcription:', { transcriptionId, updates });

      const { data, error } = await supabase
        .from('transcriptions')
        .update(updates)
        .eq('id', transcriptionId)
        .select()
        .single();

      if (error) {
        console.error('‚ùå Database error updating transcription:', error);
        throw error;
      }

      console.log('‚úÖ Transcription updated successfully');
      return data;
    } catch (error) {
      console.error('‚ùå Failed to update transcription:', error);
      throw error;
    }
  }

  /**
   * Create a real-time transcription service instance
   * This enables live transcription during recording
   */
  static createRealtimeService(options: {
    onTranscriptDelta?: (transcript: string) => void;
    onTranscriptComplete?: (transcript: string) => void;
    onError?: (error: string) => void;
    onConnect?: () => void;
    onDisconnect?: () => void;
    language?: string;
  }): RealtimeTranscriptionService {
    console.log('üé§ Creating real-time transcription service');

    return new RealtimeTranscriptionService({
      onTranscriptDelta: options.onTranscriptDelta,
      onTranscriptComplete: options.onTranscriptComplete,
      onError: options.onError,
      onConnect: options.onConnect,
      onDisconnect: options.onDisconnect
    });
  }

  /**
   * Start real-time transcription session
   */
  static async startRealtimeTranscription(
    service: RealtimeTranscriptionService,
    options: RealtimeTranscriptionOptions = {}
  ): Promise<void> {
    try {
      console.log('üéôÔ∏è Starting real-time transcription session');

      // Set default language if not specified
      if (!options.language) {
        options.language = 'fr'; // Default to French as per user's preference
      }

      await service.connect(options);
      console.log('‚úÖ Real-time transcription session started');
    } catch (error) {
      console.error('‚ùå Failed to start real-time transcription:', error);
      throw error;
    }
  }

  /**
   * Stop real-time transcription session
   */
  static stopRealtimeTranscription(service: RealtimeTranscriptionService): void {
    try {
      console.log('üõë Stopping real-time transcription session');
      service.disconnect();
      console.log('‚úÖ Real-time transcription session stopped');
    } catch (error) {
      console.error('‚ùå Failed to stop real-time transcription:', error);
    }
  }
}